{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UniVR-DH/ADHLab/blob/main/lecture01-solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Crawling with Beautifulsoup4 and  Wikipedia Python APIs to create a document collection\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1m_EMdnI5C826kgqK7r5vB4TXnB0-Wq7W\" alt=\"Intestazione con loghi istituzionali\" width=\"525\"/>\n",
        "\n",
        "| Docente      | Insegnamento | Anno Accademico    |\n",
        "| :---        |    :----   |          ---: |\n",
        "| Matteo Lissandrini      | Laboratorio Avanzato di Informatica Umanistica       | 2023/2024   |"
      ],
      "metadata": {
        "id": "4MNB7PsQ99Bx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing additional packages"
      ],
      "metadata": {
        "id": "0iPoLBUQPSsc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install wikipedia-api\n",
        "%pip install beautifulsoup4\n",
        "%pip install nltk"
      ],
      "metadata": {
        "id": "pmOC8TtR5Wm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing some basic required packages"
      ],
      "metadata": {
        "id": "M1Fok_kRPbQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gzip\n",
        "import string\n",
        "import numpy as np\n",
        "import requests\n",
        "import regex as re"
      ],
      "metadata": {
        "id": "miPm486TF629"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Crawling content with Beautifulsoup4\n",
        "#### Select a webpage, download its content, parse the HTML to extract the text"
      ],
      "metadata": {
        "id": "oZhJyAA7U5B0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "page = requests.get('https://en.wikipedia.org/wiki/New_York_City')\n",
        "\n",
        "# Create a BeautifulSoup object\n",
        "soup = BeautifulSoup(page.text, 'html.parser')\n",
        "\n",
        "# Pull text from all instances of <p> tag within `mw-body` div\n",
        "# `mw-body`  has been selected by manually inspecting the HTML code of the page\n",
        "all_p_items = soup.find(class_='mw-body').find_all('p')\n",
        "print(len(all_p_items))\n",
        "print(all_p_items[0])\n",
        "print(all_p_items[0].get_text())\n",
        "print('    ----    ')\n",
        "print(all_p_items[1])\n",
        "print(all_p_items[1].get_text())"
      ],
      "metadata": {
        "id": "5NC2R7f5PtwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# More about Regex:\n",
        "# - https://en.wikipedia.org/wiki/Regular_expression\n",
        "# - https://web.stanford.edu/~jurafsky/slp3/2.pdf\n",
        "# - https://regexone.com/\n",
        "punct_regex = re.compile('[{}]'.format(re.escape(string.punctuation))) # Regex matching any punctuation\n",
        "space_regex = re.compile(' +') # Regex matching whitespace"
      ],
      "metadata": {
        "id": "3dqnUBfTj4Om"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Example of the effect of Regexp and `strip` method"
      ],
      "metadata": {
        "id": "LERKF3dbjoaj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "test_string = \"??This . is A test String!!\"\n",
        "test_string_parsed1 = punct_regex.sub(' ', test_string)\n",
        "print(\"(1)\",\"'\"+test_string_parsed1+\"'\") # <- I am adding quotes around it\n",
        "\n",
        "test_string_parsed2 = space_regex.sub(' ', test_string_parsed1)\n",
        "print(\"(2)\", \"'\"+test_string_parsed2+\"'\") # <- I am adding quotes around it\n",
        "\n",
        "test_string_parsed3 = test_string_parsed2.strip()\n",
        "print(\"(3)\", \"'\"+test_string_parsed3+\"'\") # <- I am adding quotes around it"
      ],
      "metadata": {
        "id": "tJjalkcaSYyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Use regext to clean the lines"
      ],
      "metadata": {
        "id": "Ix1EL2kHjubu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find anything matching the regexp above in the text, then replace it with a *single* empty space\n",
        "text = punct_regex.sub(' ', soup.find(class_='mw-body').get_text())\n",
        "text = space_regex.sub(' ', text).lower()  # convert to lowercase\n",
        "lines = [ # Store lines in a list based on the newline symbol \\n\n",
        "    line.strip()\n",
        "    for line in text.split(\"\\n\")\n",
        "    if line.strip() != \"\" # Skip empty lines\n",
        "]\n",
        "\n",
        "print(len(lines))\n",
        "print(lines[0])\n",
        "print(lines[1])\n",
        "print(lines[1290])"
      ],
      "metadata": {
        "id": "Sfvb8pulI3u1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######\n",
        "# TODO: Open the wikipedia page for New York, select a sentence, can you find at which line it appears?\n",
        "######\n",
        "\n",
        "f = 'substantially by human intervention'  #<-- example of selected sentence\n",
        "\n",
        "for pos, line in enumerate(lines):\n",
        "  if f in line:\n",
        "    print(pos, \":\", line)\n"
      ],
      "metadata": {
        "id": "UQFpTUapUG07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######\n",
        "# TODO: Complete the code,\n",
        "#   a) split a line in single words, compute word frequency\n",
        "#   b) compute word frequency of all words across all lines\n",
        "#\n",
        "# Try out: https://docs.python.org/3/library/collections.html#collections.Counter\n",
        "#\n",
        "######\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "# Split the line when we find a space ' ' symbol\n",
        "word_list = lines[1290].split(' ')\n",
        "print(len(word_list))\n",
        "\n",
        "# making a set is not a good option, it removes duplicates\n",
        "words = set( w for w in word_list)\n",
        "print(len(words))\n",
        "print(words)\n",
        "\n",
        "# Use the Counter object to count frequencies\n",
        "word_count = Counter(word_list)\n",
        "print(word_count)\n",
        "\n",
        "#word_count.most_common(2)\n",
        "\n",
        "# To compute word frequency of all words across all lines\n",
        "# we iterate line by line and keep updating the same Counter object\n",
        "word_count = Counter()\n",
        "for line in lines:\n",
        "  word_count.update(line.split(' '))\n",
        "\n",
        "print(len(word_count))\n",
        "# most_common can print the most common words found\n",
        "word_count.most_common(10)\n"
      ],
      "metadata": {
        "id": "Gi4hqd9sinL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Accessing Links in the page"
      ],
      "metadata": {
        "id": "1F_ewMNEVA2_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We search for the <a> tags in the HTML of the page\n",
        "all_a_items = soup.find(class_='mw-body').find_all('a')\n",
        "print(len(all_a_items))\n",
        "\n",
        "# We print the actual URL to which they point\n",
        "for a in all_a_items:\n",
        "  href = a.get('href')\n",
        "  if href is not None and href.startswith('/wiki/') and not 'File:' in href:\n",
        "    # we only print those URLs that start with `/wiki/` because they are internal to Wikipedia\n",
        "    # but we exclude those that point to Files -- e.g., images\n",
        "    print(href)"
      ],
      "metadata": {
        "id": "oZgJPxHhQ9F7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######\n",
        "# TODO: Create a dictionary of /wiki/ links, and count how many times they appear in the page, which are the top-5 most frequent links?\n",
        "######\n",
        "\n",
        "wiki_links = []\n",
        "wiki_links_count = Counter()\n",
        "\n",
        "for a in all_a_items:\n",
        "  href = a.get('href')\n",
        "  if href is not None and href.startswith('/wiki/') and not 'File:' in href:\n",
        "    # we only print those URLs that start with `/wiki/` because they are internal to Wikipedia\n",
        "    # but we exclude those that point to Files -- e.g., images\n",
        "    wiki_links.append(href)\n",
        "\n",
        "wiki_links_count = Counter(wiki_links)\n",
        "display(wiki_links_count.most_common(5))"
      ],
      "metadata": {
        "id": "h1-fH47yRtkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######\n",
        "# TODO: Pick the most frequent /wiki/ link from the above dictionary,\n",
        "# download its page content and extract all links,\n",
        "# do you find links in common ?\n",
        "######\n",
        "\n",
        "\n",
        "# Most frequent wiki link from the counter\n",
        "most_frequent = wiki_links_count.most_common(1)[0][0] #<-- why I need a [0][0]?\n",
        "print(most_frequent)\n",
        "\n",
        "# Download its page content\n",
        "page2 = requests.get('https://en.wikipedia.org/'+most_frequent)\n",
        "\n",
        "\n",
        "# Extract all links\n",
        "soup2 = BeautifulSoup(page2.text, 'html.parser')\n",
        "all_a_items2 = soup2.find(class_='mw-body').find_all('a')\n",
        "\n",
        "wiki_links2 = []\n",
        "wiki_links_count2 = Counter()\n",
        "\n",
        "for a in all_a_items2:\n",
        "  href = a.get('href')\n",
        "  if href is not None and href.startswith('/wiki/') and not 'File:' in href:\n",
        "    # we only print those URLs that start with `/wiki/` because they are internal to Wikipedia\n",
        "    # but we exclude those that point to Files -- e.g., images\n",
        "    wiki_links2.append(href)\n",
        "\n",
        "# Display most common links\n",
        "wiki_links_count2 = Counter(wiki_links2)\n",
        "display(wiki_links_count2.most_common(5))"
      ],
      "metadata": {
        "id": "x7UTbGzLUd4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "links_page1 = wiki_links_count.keys()\n",
        "\n",
        "links_page2 = wiki_links_count2.keys()\n",
        "\n",
        "display(links_page1 & links_page2) # <- set intersection"
      ],
      "metadata": {
        "id": "g2s6PhnHPHui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### For those that have not seen '&' as a set intersection operator:\n",
        "\n",
        "> Indented block\n",
        "\n"
      ],
      "metadata": {
        "id": "eoYf73d6jfVE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "company_names = set(['apple', 'amazon', 'adobe', 'shell', 'orange', 'lotus', 'microsoft'])\n",
        "organic_objects = set(['apple', 'banana', 'mango', 'rose', 'shell', 'orange', 'lotus'])\n",
        "\n",
        "print(company_names & organic_objects)"
      ],
      "metadata": {
        "id": "v3LcX_o2PtV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract content from Wikipedia with the Wikipedia APIs"
      ],
      "metadata": {
        "id": "nkAYwP-5U0rU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipediaapi\n",
        "# Creates an object able to connect to Wikipedia and download\n",
        "# directly parsed content in text form without HTML tags\n",
        "## EDIT Down There: put your name and email for the Wikipedia logs\n",
        "wapi_text = wikipediaapi.Wikipedia('MyTestProjectName (my.name@univr.it)',\n",
        "                                   'en',\n",
        "                                   extract_format=wikipediaapi.ExtractFormat.WIKI)"
      ],
      "metadata": {
        "id": "f2ymnwhM7PGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "page_py = wapi_text.page('New York City')\n",
        "print(\"Page - Exists: {}\".format( page_py.exists()))\n",
        "print(len(page_py.summary))\n",
        "print(len(page_py.text))\n",
        "print(len(page_py.langlinks))\n",
        "print(len(page_py.links))"
      ],
      "metadata": {
        "id": "9peoLRdX5yfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(page_py.summary[:140])\n",
        "print(\"   ---   \")\n",
        "\n",
        "print(page_py.text[-140:])\n",
        "print(\"   ---   \")\n",
        "\n",
        "print(sorted(page_py.langlinks.keys()))\n",
        "print(\"   ---   \")\n",
        "\n",
        "page_py_it = page_py.langlinks['it']  # <-- this links to the same page in the italian Wikipedia\n",
        "print(page_py_it.summary[:140])"
      ],
      "metadata": {
        "id": "pK-HhvuAOBXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we go over all links in the original page (english page)\n",
        "links = page_py.links\n",
        "for title in sorted(links.keys()): # each link has a Title\n",
        "    if len(title) > 4 : # filter on title length to reduce output\n",
        "      continue\n",
        "    print(\"{}\".format(title))"
      ],
      "metadata": {
        "id": "DmrRNRqTOcRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_pages = ['Addis Ababa',  'Tom Sawyer', 'Johannes Gutenberg']"
      ],
      "metadata": {
        "id": "dBmBbb925bLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.parse import quote\n",
        "from collections import deque\n",
        "\n",
        "# Example of simple crawling code\n",
        "\n",
        "## make a queue of pages initialized to some pages of interest\n",
        "page_queue = deque( wapi_text.page(tp) for  tp in test_pages )\n",
        "\n",
        "# prepare auxiliar data structures\n",
        "page_stored = {}\n",
        "page_visited = set()\n",
        "\n",
        "# we put a limit to stop after a bit to avoid downloading too many pages\n",
        "max_iterations = 10\n",
        "\n",
        "\n",
        "# Here starts the \"crawling\" which will continue untile we have pages to extract\n",
        "#   or until we do not reach max num of iterations\n",
        "while len(page_queue) > 0 and max_iterations > 0:\n",
        "\n",
        "  # get the first page in the queue\n",
        "  _page = page_queue.popleft()\n",
        "\n",
        "  # save its full text\n",
        "  page_stored[_page.fullurl] = _page.text\n",
        "\n",
        "  # we add it to the set of pages visited (to avoid visiting it again)\n",
        "  page_visited.add(_page.fullurl)\n",
        "\n",
        "  # just print something to show progress\n",
        "  print(max_iterations, _page.title, _page.fullurl)\n",
        "\n",
        "  # Update the number of iterations\n",
        "  max_iterations = max_iterations - 1\n",
        "\n",
        "  # Find all links, add *some* of those pages to the queue for crawling\n",
        "  for next_page in _page.links.values():\n",
        "    try: # We use a \"try\" to skip in case of errors\n",
        "      if not next_page.exists():\n",
        "        continue\n",
        "\n",
        "      _next_page_url = next_page.fullurl\n",
        "      _next_page_url_fragment = _next_page_url.split('/')[-1]\n",
        "      if len(_next_page_url_fragment) < 6 and len(_next_page_url_fragment) > 13: # just a random filter to avoid downloading too many pages\n",
        "        continue # skip this page\n",
        "\n",
        "      # of course, if a page has been already visited, we ignore that link\n",
        "      if _next_page_url in page_visited:\n",
        "        continue # skip this page\n",
        "\n",
        "      # otherwise we add it to the pages to visit next\n",
        "      page_queue.append(next_page)\n",
        "    except Exception as ex:\n",
        "      print(\"\\tError retrieving\", next_page.title)\n",
        "\n",
        "\n",
        "print(len(page_stored))\n",
        "print(page_stored.keys())"
      ],
      "metadata": {
        "id": "fQSrh1c2Vp-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######\n",
        "# TODO: Create the bag of words for all page texts, remember to transform the text in lowercase and remove punctuation\n",
        "######\n",
        "\n",
        "# regexp from above\n",
        "punct_regex = re.compile('[{}]'.format(re.escape(string.punctuation))) # Regex matching any punctuation\n",
        "space_regex = re.compile(' +') # Regex matching whitespace\n",
        "\n",
        "\n",
        "# page_stored[_page.fullurl] = _page.text\n",
        "page_bow = {}\n",
        "for url, text in page_stored.items():\n",
        "  page_bow[url] = Counter(space_regex.sub(' ', punct_regex.sub(' ', text.lower())).strip().split())\n"
      ],
      "metadata": {
        "id": "pa8-zAiMk2sV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for url, bow in page_bow.items():\n",
        "  print(url, bow.most_common(5))"
      ],
      "metadata": {
        "id": "5OMi2rtBesDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The following declaration extract unparsed HTML instead of already parsed text"
      ],
      "metadata": {
        "id": "DLpeEL5_VyYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wapi_html = wikipediaapi.Wikipedia('MyProjectName (name@studenti.univr.it)',\n",
        "                              'en',\n",
        "                              extract_format=wikipediaapi.ExtractFormat.HTML)\n",
        "page_py = wapi_text.page('New York City')\n",
        "print(\"Page - Exists: {}\".format( page_py.exists()))\n",
        "print(len(page_py.summary))\n"
      ],
      "metadata": {
        "id": "OW8WNKmAUOKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stemming and lemmatization"
      ],
      "metadata": {
        "id": "CRPkFNA_la_S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer\n",
        "\n",
        "\n",
        "## Download resources needed by methods\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"omw-1.4\")\n",
        "\n",
        "# More info:\n",
        "# -- https://www.nltk.org/howto/stem.html\n",
        "# -- https://www.nltk.org/howto/wordnet.html"
      ],
      "metadata": {
        "id": "a-XhpJaflOAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Python porter stemmer\n",
        "ps = PorterStemmer()\n",
        "sn = SnowballStemmer(\"english\")\n",
        "\n",
        "example_sentence = \"\"\"Programming is an art and a job.\n",
        "Python programmers often tend to like programming in python\n",
        "because it's like english.\n",
        "This is a better language than many others and an incredibly\n",
        "useful property that makes things easier.\n",
        "We called people who program in python pythonistas.\"\"\"\n",
        "\n",
        "# Remove punctuation\n",
        "example_sentence_no_punct = example_sentence.lower().translate(\n",
        "    str.maketrans(\"\", \"\", string.punctuation)\n",
        "    )\n",
        "\n",
        "# Create tokens\n",
        "word_tokens = word_tokenize(example_sentence_no_punct)\n",
        "\n",
        "# Perform stemming\n",
        "print(\"{0:20}{1:20}\".format(\"--Word--\",\"--Stem--\"))\n",
        "for word in word_tokens:\n",
        "    print (\"{0:20}{1:20}{2:20}\".format(word, ps.stem(word), sn.stem(word)))\n"
      ],
      "metadata": {
        "id": "GsJe2dKErAWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize wordnet lemmatizer\n",
        "wnl = WordNetLemmatizer()\n",
        "\n",
        "# wn.VERB\n",
        "# wn.ADV\n",
        "# wn.NOUN\n",
        "\n",
        "# Perform lemmatization\n",
        "print(\"{0:6}{1:20}{1:20}\".format(\"POS\",\"--Word--\",\"--Lemma--\"))\n",
        "for word in word_tokens:\n",
        "  lemmatized = wnl.lemmatize(word, pos=wordnet.ADJ)\n",
        "  if word != lemmatized:\n",
        "    print (\"{0:6}{1:20}{2:20}\".format(\"ADJ\", word, lemmatized)) # <- lemmatize as if they are all adjectives\n",
        "\n",
        "  lemmatized = wnl.lemmatize(word, pos=wordnet.VERB)\n",
        "  if word != lemmatized:\n",
        "    print (\"{0:6}{1:20}{2:20}\".format(\"VERB\", word, lemmatized)) # <- lemmatize as if they are all adjectives\n",
        "\n",
        "  lemmatized = wnl.lemmatize(word, pos=wordnet.ADV)\n",
        "  if word != lemmatized:\n",
        "    print (\"{0:6}{1:20}{2:20}\".format(\"ADV\", word, lemmatized)) # <- lemmatize as if they are all adjectives\n"
      ],
      "metadata": {
        "id": "u_naQ1fJn91E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######\n",
        "# TODO: Text stemming and lemmatization with a wikipedia page summary\n",
        "######\n",
        "\n"
      ],
      "metadata": {
        "id": "Y5H2bN1dV2nq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}