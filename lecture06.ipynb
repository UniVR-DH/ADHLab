{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UniVR-DH/ADHLab/blob/main/lecture06.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing Named Entity Recognition\n",
        "\n",
        "We will use the Spacy Library:\n",
        "https://spacy.io/usage/spacy-101\n"
      ],
      "metadata": {
        "id": "4MNB7PsQ99Bx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1m_EMdnI5C826kgqK7r5vB4TXnB0-Wq7W\" alt=\"Intestazione con loghi istituzionali\" width=\"525\"/>\n",
        "\n",
        "| Docente      | Insegnamento | Anno Accademico    |\n",
        "| :---        |    :----   |          ---: |\n",
        "| Matteo Lissandrini      | Laboratorio Avanzato di Informatica Umanistica       | 2023/2024   |"
      ],
      "metadata": {
        "id": "dChTXqspd6TT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Usual install and basic imports"
      ],
      "metadata": {
        "id": "GXBM6I9FeBmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install wikipedia-api\n",
        "%pip install spacy==3.7.0"
      ],
      "metadata": {
        "id": "pmOC8TtR5Wm5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb1530c6-9a0d-40f2-f605-369bb36d6d90"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wikipedia-api\n",
            "  Downloading Wikipedia_API-0.6.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from wikipedia-api) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (2023.11.17)\n",
            "Installing collected packages: wikipedia-api\n",
            "Successfully installed wikipedia-api-0.6.0\n",
            "Collecting spacy==3.7.0\n",
            "  Downloading spacy-3.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.0) (2.0.10)\n",
            "Collecting weasel<0.4.0,>=0.1.0 (from spacy==3.7.0)\n",
            "  Downloading weasel-0.3.4-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.0) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.0) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.0) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.0) (1.23.5)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.0) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy==3.7.0) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy==3.7.0) (8.1.7)\n",
            "Collecting cloudpathlib<0.17.0,>=0.7.0 (from weasel<0.4.0,>=0.1.0->spacy==3.7.0)\n",
            "  Downloading cloudpathlib-0.16.0-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy==3.7.0) (2.1.3)\n",
            "Installing collected packages: cloudpathlib, weasel, spacy\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.6.1\n",
            "    Uninstalling spacy-3.6.1:\n",
            "      Successfully uninstalled spacy-3.6.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "en-core-web-sm 3.6.0 requires spacy<3.7.0,>=3.6.0, but you have spacy 3.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed cloudpathlib-0.16.0 spacy-3.7.0 weasel-0.3.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipediaapi\n",
        "import re\n",
        "\n",
        "# Step 1: Import necessary libraries\n",
        "#wiki_wiki = wikipediaapi.Wikipedia('en', user_agent='YourUserAgent/1.0')  # WRONG\n",
        "# wiki_wiki = wikipediaapi.Wikipedia('en', extract_format=wikipediaapi.ExtractFormat.WIKI, user_agent='YourUserAgent/1.0') # WRONG\n",
        "wiki_wiki = wikipediaapi.Wikipedia('MyTestProjectName (my.name@univr.it)',\n",
        "                                   'en',\n",
        "                                   extract_format=wikipediaapi.ExtractFormat.WIKI)\n",
        "\n",
        "# Step 2: Prepare a list of titles of Wikipedia pages\n",
        "fantasy_literature_titles = [\n",
        "    \"Fantasy literature\",\n",
        "    \"The Lord of the Rings\",\n",
        "    \"Harry Potter\",\n",
        "    \"A Song of Ice and Fire\",\n",
        "    \"The Chronicles of Narnia\",\n",
        "    \"The Hobbit\",\n",
        "    \"Alice's Adventures in Wonderland\",\n",
        "    \"The Wizard of Oz\",\n",
        "    \"The Silmarillion\",\n",
        "    \"Discworld\",\n",
        "    \"His Dark Materials\",\n",
        "    \"The Wheel of Time\",\n",
        "    \"Earthsea\",\n",
        "    \"The Once and Future King\",\n",
        "    \"The Princess Bride\",\n",
        "    \"The Name of the Wind\",\n",
        "    \"Mistborn\",\n",
        "    \"The Malazan Book of the Fallen\",\n",
        "    \"The Kingkiller Chronicle\",\n",
        "    \"The Inheritance Cycle\"\n",
        "]\n",
        "\n",
        "# Step 3: Write code to download main content in plain text and create a collection of cleaned text\n",
        "cleaned_texts = []\n",
        "\n",
        "for title in fantasy_literature_titles:\n",
        "    page_py = wiki_wiki.page(title)\n",
        "\n",
        "    if page_py.exists():\n",
        "        # Keep the original URL of the page\n",
        "        original_url = page_py.fullurl\n",
        "\n",
        "        # Remove brackets and their contents from the text\n",
        "        content = re.sub(r'\\[[^\\]]*\\]', '', page_py.text)\n",
        "\n",
        "        # Remove newline characters and extra spaces\n",
        "        clean_text = ' '.join(content.split())\n",
        "\n",
        "        # Store the cleaned text along with the original URL\n",
        "        cleaned_texts.append({'title': title, 'content': clean_text, 'url': original_url})\n",
        "    else:\n",
        "        print(f\"Page '{title}' does not exist on Wikipedia.\")\n",
        "\n",
        "# Display the cleaned texts and their original URLs (optional)\n",
        "# for idx, entry in enumerate(cleaned_texts, 1):\n",
        "#     print(f\"Text {idx} - Title: {entry['title']}\\nURL: {entry['url']}\\nContent:\\n{entry['content']}\\n{'='*50}\\n\")\n"
      ],
      "metadata": {
        "id": "r55SvyEnxVWZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import re\n",
        "\n",
        "# Step 4: Build an inverted index of lowercase ngrams ignoring non-alphanumeric symbols\n",
        "inverted_index = defaultdict(list)\n",
        "\n",
        "NGRAM_SIZE = 5\n",
        "\n",
        "for entry in cleaned_texts:\n",
        "    title = entry['title']\n",
        "    content = entry['content'].lower()\n",
        "    url = entry['url']\n",
        "\n",
        "    # Remove non-alphanumeric symbols\n",
        "    content = re.sub(r'[^a-z0-9 ]', '', content)\n",
        "\n",
        "    # Generate ngrams of size NGRAM_SIZE\n",
        "    ngrams = [content[i:i+NGRAM_SIZE] for i in range(len(content)-(NGRAM_SIZE-1))]\n",
        "\n",
        "    # Build inverted index\n",
        "    for trigram in set(ngrams):  # Using set to remove duplicates\n",
        "        inverted_index[trigram].append({'title': title, 'url': url})\n",
        "\n",
        "# Display the inverted index (optional)\n",
        "# for ngram, entries in inverted_index.items():\n",
        "#     print(f\"N-gram: {ngram}\")\n",
        "#     for entry in entries:\n",
        "#         print(f\"  Title: {entry['title']}, URL: {entry['url']}\")\n"
      ],
      "metadata": {
        "id": "9r6VqaaPxl-D"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def compute_tfidf(query, inverted_index, cleaned_texts):\n",
        "    # Step 5: Given an input query, compute the TF-IDF for each page that matches at least one ngram\n",
        "    query = query.lower()\n",
        "\n",
        "    # Remove non-alphanumeric symbols\n",
        "    query = re.sub(r'[^a-z0-9 ]', '', query)\n",
        "\n",
        "    # Generate ngrams\n",
        "    query_ngrams = [query[i:i+NGRAM_SIZE] for i in range(len(query)-(NGRAM_SIZE-1))]\n",
        "\n",
        "    # Find matching trigrams in the inverted index\n",
        "    matching_ngrams = set(query_ngrams).intersection(inverted_index.keys())\n",
        "\n",
        "    # Collect titles and URLs of pages matching the trigrams\n",
        "    matching_pages = []\n",
        "    for ngram in matching_ngrams:\n",
        "        matching_pages.extend(inverted_index[ngram])\n",
        "\n",
        "    # Get unique titles and URLs\n",
        "    unique_titles = list({page['title'] for page in matching_pages})\n",
        "    unique_urls = list({page['url'] for page in matching_pages})\n",
        "\n",
        "    # Extract content for the matching pages\n",
        "    matching_contents = [entry['content'] for entry in cleaned_texts if entry['title'] in unique_titles]\n",
        "\n",
        "    # Compute TF-IDF\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(matching_contents)\n",
        "\n",
        "    # Get feature names (words) from the vectorizer\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "    # Prepare a list of pairs with URL and TF-IDF score for each matching page\n",
        "    result_list = []\n",
        "    for i, title in enumerate(unique_titles):\n",
        "        tfidf_values = tfidf_matrix[i].toarray()[0]\n",
        "        page_tfidf = {feature_names[j]: tfidf_values[j] for j in range(len(feature_names))}\n",
        "        result_list.append({'url': unique_urls[i], 'tfidf_score': page_tfidf})\n",
        "\n",
        "    return result_list\n",
        "\n",
        "# Example usage:\n",
        "query = \"fantasy adventure\"\n",
        "result = compute_tfidf(query, inverted_index, cleaned_texts)\n",
        "print(len(result))\n"
      ],
      "metadata": {
        "id": "JJS2Lw6xzAcF",
        "outputId": "57dc2789-28cc-474a-bd34-92bbe5c3fafe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to print the top-k URLs and their TF-IDF scores\n",
        "def print_top_k_urls(result_list, k):\n",
        "    # Sort the result list based on the sum of TF-IDF scores\n",
        "    sorted_results = sorted(result_list, key=lambda x: sum(x['tfidf_score'].values()), reverse=True)\n",
        "\n",
        "    # Print the top-k URLs and their TF-IDF scores\n",
        "    for i in range(min(k, len(sorted_results))):\n",
        "        url = sorted_results[i]['url']\n",
        "        tfidf_value = sum(sorted_results[i]['tfidf_score'].values())\n",
        "        print(f\"Top {i+1} URL: {url}, TF-IDF Value: {tfidf_value}\")\n",
        "\n",
        "# Example usage:\n",
        "query = \"fantasy adventure\"\n",
        "k = 5  # Change k to the desired number of top URLs\n",
        "result = compute_tfidf(query, inverted_index, cleaned_texts)\n",
        "print_top_k_urls(result, k)\n"
      ],
      "metadata": {
        "id": "Yzyo0rRtZn87",
        "outputId": "8d42001d-417d-4475-9a0b-8fd984a1114f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 1 URL: https://en.wikipedia.org/wiki/Earthsea, TF-IDF Value: 15.266600695829172\n",
            "Top 2 URL: https://en.wikipedia.org/wiki/The_Name_of_the_Wind, TF-IDF Value: 14.107094592697385\n",
            "Top 3 URL: https://en.wikipedia.org/wiki/The_Once_and_Future_King, TF-IDF Value: 13.80408119729509\n",
            "Top 4 URL: https://en.wikipedia.org/wiki/The_Silmarillion, TF-IDF Value: 13.661325773324807\n",
            "Top 5 URL: https://en.wikipedia.org/wiki/The_Inheritance_Cycle, TF-IDF Value: 13.65790134661919\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kwOB64JSbVfu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}