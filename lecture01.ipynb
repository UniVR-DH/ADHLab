{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UniVR-DH/ADHLab/blob/main/lecture01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Crawling with Beautifulsoup4 and  Wikipedia Python APIs to create a document collection\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1m_EMdnI5C826kgqK7r5vB4TXnB0-Wq7W\" alt=\"Intestazione con loghi istituzionali\" width=\"525\"/>\n",
        "\n",
        "| Docente      | Insegnamento | Anno Accademico    |\n",
        "| :---        |    :----   |          ---: |\n",
        "| Matteo Lissandrini      | Laboratorio Avanzato di Informatica Umanistica       | 2023/2024   |"
      ],
      "metadata": {
        "id": "4MNB7PsQ99Bx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing additional packages"
      ],
      "metadata": {
        "id": "0iPoLBUQPSsc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install wikipedia-api\n",
        "%pip install beautifulsoup4\n",
        "%pip install nltk"
      ],
      "metadata": {
        "id": "pmOC8TtR5Wm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing some basic required packages"
      ],
      "metadata": {
        "id": "M1Fok_kRPbQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gzip\n",
        "import string\n",
        "import numpy as np\n",
        "import requests\n",
        "import regex as re"
      ],
      "metadata": {
        "id": "miPm486TF629"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Crawling content with Beautifulsoup4\n",
        "#### Select a webpage, download its content, parse the HTML to extract the text"
      ],
      "metadata": {
        "id": "oZhJyAA7U5B0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "page = requests.get('https://en.wikipedia.org/wiki/New_York_City')\n",
        "\n",
        "# Create a BeautifulSoup object\n",
        "soup = BeautifulSoup(page.text, 'html.parser')\n",
        "\n",
        "# Pull text from all instances of <p> tag within BodyText div\n",
        "all_p_items = soup.find(class_='mw-body').find_all('p')\n",
        "print(len(all_p_items))\n",
        "print(all_p_items[0])\n",
        "print(all_p_items[0].get_text())\n",
        "print('    ----    ')\n",
        "print(all_p_items[1])\n",
        "print(all_p_items[1].get_text())"
      ],
      "metadata": {
        "id": "5NC2R7f5PtwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punct_regex = re.compile('[{}]'.format(re.escape(string.punctuation))) # Regex matching any punctuation\n",
        "space_regex = re.compile(' +') # Regex matching whitespace\n",
        "\n",
        "text = punct_regex.sub(' ', soup.find(class_='mw-body').get_text())\n",
        "text = space_regex.sub(' ', text).lower()  # convert to lowercase\n",
        "lines = [\n",
        "    line.strip()\n",
        "    for line in text.split(\"\\n\")\n",
        "    if line.strip() != \"\" # Skip empty lines\n",
        "]\n",
        "# Store lines\n",
        "print(len(lines))\n",
        "print(lines[0])\n",
        "print(lines[1])\n",
        "print(lines[1290])"
      ],
      "metadata": {
        "id": "tJjalkcaSYyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######\n",
        "# TODO: Open the wikipedia page for New York, select a sentence, can you find at which line it appears?\n",
        "######\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UQFpTUapUG07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######\n",
        "# TODO: Complete the code,\n",
        "#   a) split a line in single words, compute word frequency\n",
        "#   b) compute word frequency of all words across all lines\n",
        "#\n",
        "# Try out: https://docs.python.org/3/library/collections.html#collections.Counter\n",
        "#\n",
        "######\n",
        "\n",
        "print(len(lines[1290].split(' ')))\n",
        "words = set( w for w in lines[1290].split(' '))\n",
        "print(len(words))\n",
        "print(words)\n"
      ],
      "metadata": {
        "id": "Gi4hqd9sinL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Accessing Links in the page"
      ],
      "metadata": {
        "id": "1F_ewMNEVA2_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_a_items = soup.find(class_='mw-body').find_all('a')\n",
        "print(len(all_a_items))\n",
        "for a in all_a_items:\n",
        "  href = a.get('href')\n",
        "  if href is not None and href.startswith('/wiki/') :\n",
        "    print(href)"
      ],
      "metadata": {
        "id": "oZgJPxHhQ9F7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######\n",
        "# TODO: Create a dictionary of /wiki/ links, and count how many times they appear in the page, which are the top-5 most frequent links?\n",
        "######\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "h1-fH47yRtkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######\n",
        "# TODO: Pick the most frequent /wiki/ link from the above dictionary,\n",
        "# download its page content and extract all links,\n",
        "# do you find links in common ?\n",
        "######\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "x7UTbGzLUd4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract content from Wikipedia with the Wikipedia APIs"
      ],
      "metadata": {
        "id": "nkAYwP-5U0rU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipediaapi\n",
        "## EDIT Down There: put your name and email for the Wikipedia logs\n",
        "wapi_text = wikipediaapi.Wikipedia('MyProjectName (name@studenti.univr.it)',\n",
        "                                   'en',\n",
        "                                   extract_format=wikipediaapi.ExtractFormat.WIKI)"
      ],
      "metadata": {
        "id": "f2ymnwhM7PGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "page_py = wapi_text.page('New York City')\n",
        "print(\"Page - Exists: {}\".format( page_py.exists()))\n",
        "print(len(page_py.summary))\n",
        "print(len(page_py.text))\n",
        "print(len(page_py.langlinks))\n",
        "print(len(page_py.links))"
      ],
      "metadata": {
        "id": "9peoLRdX5yfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(page_py.summary[:140])\n",
        "print(\"   ---   \")\n",
        "print(page_py.text[-140:])\n",
        "print(\"   ---   \")\n",
        "print(sorted(page_py.langlinks.keys()))\n",
        "print(\"   ---   \")\n",
        "page_py_it = page_py.langlinks['it']\n",
        "print(page_py_it.summary[:140])"
      ],
      "metadata": {
        "id": "pK-HhvuAOBXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "links = page_py.links\n",
        "for title in sorted(links.keys()):\n",
        "    if len(title) > 4 : # filter on title length to reduce output\n",
        "      continue\n",
        "    print(\"{}\".format(title))"
      ],
      "metadata": {
        "id": "DmrRNRqTOcRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_pages = ['Addis Ababa',  'Tom Sawyer', 'Johannes Gutenberg']"
      ],
      "metadata": {
        "id": "dBmBbb925bLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.parse import quote\n",
        "\n",
        "page_queue = [ wapi_text.page(tp) for  tp in test_pages ]\n",
        "page_stored = {}\n",
        "page_visited = set()\n",
        "max_iterations = 50\n",
        "\n",
        "while len(page_queue) > 0 and max_iterations > 0:\n",
        "  _page = page_queue.pop()\n",
        "\n",
        "  page_stored[_page.fullurl] = _page.summary\n",
        "\n",
        "  page_visited.add(_page.fullurl)\n",
        "\n",
        "  print(max_iterations, _page.title, _page.fullurl)\n",
        "  max_iterations = max_iterations - 1\n",
        "\n",
        "  for next_page in _page.links.values():\n",
        "    try:\n",
        "      if len(next_page.title) < 6 and len(next_page.title) > 13:\n",
        "        continue # skip this page\n",
        "\n",
        "      if ':' in next_page.title :\n",
        "        continue\n",
        "\n",
        "      if next_page.fullurl in page_visited:\n",
        "        continue # skip this page\n",
        "\n",
        "      # otherwise\n",
        "      page_queue.append(next_page)\n",
        "    except:\n",
        "      print(\"Error retrieving\", next_page.title)\n",
        "\n",
        "\n",
        "print(len(page_stored))\n",
        "print(page_stored.keys())"
      ],
      "metadata": {
        "id": "fQSrh1c2Vp-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "page_stored['https://en.wikipedia.org/wiki/Victoria_Falls_Airport']"
      ],
      "metadata": {
        "id": "WiWiNnsliV0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######\n",
        "# TODO: Create the bag of words for all page summaries, remember to transform the text in lowercase and remove punctuation\n",
        "######"
      ],
      "metadata": {
        "id": "pa8-zAiMk2sV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The following declaration extract unparsed HTML instead of already parsed text"
      ],
      "metadata": {
        "id": "DLpeEL5_VyYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wapi_html = wikipediaapi.Wikipedia('MyProjectName (name@studenti.univr.it)',\n",
        "                              'en',\n",
        "                              extract_format=wikipediaapi.ExtractFormat.HTML)\n",
        "page_py = wapi_text.page('New York City')\n",
        "print(\"Page - Exists: {}\".format( page_py.exists()))\n",
        "print(len(page_py.summary))\n"
      ],
      "metadata": {
        "id": "OW8WNKmAUOKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stemming nad lemmatization"
      ],
      "metadata": {
        "id": "CRPkFNA_la_S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"omw-1.4\")"
      ],
      "metadata": {
        "id": "a-XhpJaflOAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Python porter stemmer\n",
        "ps = PorterStemmer()\n",
        "sn = SnowballStemmer(\"english\")\n",
        "\n",
        "example_sentence = \"Programming is an art and a job. Python programmers often tend to like programming in python because it's like english. This is a better language than many ohters an incredibly useful property that makes things easier. We call people who program in python pythonistas.\"\n",
        "\n",
        "# Remove punctuation\n",
        "example_sentence_no_punct = example_sentence.lower().translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "\n",
        "# Create tokens\n",
        "word_tokens = word_tokenize(example_sentence_no_punct)\n",
        "\n",
        "# Perform stemming\n",
        "print(\"{0:20}{1:20}\".format(\"--Word--\",\"--Stem--\"))\n",
        "for word in word_tokens:\n",
        "    print (\"{0:20}{1:20}{2:20}\".format(word, ps.stem(word), sn.stem(word)))\n"
      ],
      "metadata": {
        "id": "GsJe2dKErAWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize wordnet lemmatizer\n",
        "wnl = WordNetLemmatizer()\n",
        "\n",
        "# wn.VERB\n",
        "# wn.ADV\n",
        "# wn.NOUN\n",
        "\n",
        "# Perform lemmatization\n",
        "print(\"{0:20}{1:20}\".format(\"--Word--\",\"--Lemma--\"))\n",
        "for word in word_tokens:\n",
        "   print (\"{0:20}{1:20}\".format(word, wnl.lemmatize(word, pos=wordnet.ADJ))) # <- lemmatize as if they are all adjectives\n",
        ""
      ],
      "metadata": {
        "id": "u_naQ1fJn91E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######\n",
        "# TODO: Text stemming and lemmatization with a wikipedia page summary\n",
        "######\n",
        "\n"
      ],
      "metadata": {
        "id": "Y5H2bN1dV2nq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}